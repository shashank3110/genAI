{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397dad2d-fdc8-4a73-9394-ab4cfc94a816",
   "metadata": {
    "tags": []
   },
   "source": [
    "## YouTube Video Analysis on Vertex AI using PaLM and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e8b5b-bcef-4a5f-8d9c-a4c820d6579c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --user google-cloud-aiplatform>=1.29.0 google-cloud-storage langchain pytube youtube-transcript-api chromadb gradio pydantic\n",
    "# You may safely ignore dependency warnings below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddaff1c2-0c30-41ba-a2ac-1b6a1640b6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5292c22c-b1aa-4e04-83e4-e5d312324506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flist = !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea3eafd-08be-49f1-80dc-3216f23d8996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get project ID\n",
    "PROJECT_ID = ! gcloud config get project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "LOCATION = \"us-central1\" # <-- Enter assigned region here.\n",
    "\n",
    "# generate an unique id for this session\n",
    "from datetime import datetime\n",
    "UID = datetime.now().strftime(\"%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f4928-e2eb-4236-9ea4-e7c75b486a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init the vertexai package\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57760555-1830-477e-a770-4e4e89c7acf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import VertexAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6209eee-f4a3-4ea5-9155-8382e70eecf8",
   "metadata": {},
   "source": [
    "### Load PALM (bison) LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c17f158-5345-4cde-a8dd-3f3832afb478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e196067b-a943-4393-9146-cc54f3809075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import langchain.llms as LLMS_lc\n",
    "# help(LLMS_lc)\n",
    "# help(VertexAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c44dc6-eef9-4d4f-bb28-4c5bcaa4320c",
   "metadata": {},
   "source": [
    "### Load pretrained text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91bf0e0f-247a-4156-93da-805255260b08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model_name will become a required arg for VertexAIEmbeddings starting from Feb-01-2024. Currently the default is set to textembedding-gecko@001\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# Embedding\n",
    "EMBEDDING_QPM = 100\n",
    "EMBEDDING_NUM_BATCH =5\n",
    "\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b53750-18b0-4948-a304-98ef8c02e008",
   "metadata": {},
   "source": [
    "### Load YouTube video Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2db0bc4-1e9b-4232-a3ea-718a4381957b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_url = \"https://www.youtube.com/watch?v=XX2XpqklUrE\"\n",
    "\n",
    "def load_video_transcripts(url=None):\n",
    "\n",
    "    if url is None:\n",
    "        url = sample_url\n",
    "        \n",
    "    loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)\n",
    "    result = loader.load()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84126e5e-7b93-41a1-ae0c-8328eaeaecca",
   "metadata": {},
   "source": [
    "### Split transcripts into chunks and create embeddings using pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f75d17-cbf6-401b-b92b-b4e3912be9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transcript_embeddings(transcript=''):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(transcript)\n",
    "    print(f\"# of documents = {len(docs)}\")\n",
    "    \n",
    "    embedding_list = embeddings.embed_documents([doc.page_content for doc in docs])\n",
    "    print(f\"You have {len(embedding_list)} embeddings\")\n",
    "    print(f\"Here's a sample of one: {embedding_list[0][:3]}...\")\n",
    "    \n",
    "    return docs, embedding_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ee1e8-77f2-4f08-997d-21433316716f",
   "metadata": {},
   "source": [
    "### Index video-transcript as Document embeddings using a vector database here we are using Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8696a92-bf4a-41d2-9570-7ae64a4c3ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def index_and_retrieve(docs=None, embeddings=None):\n",
    "    db = Chroma.from_documents(docs, embeddings)\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) # here search_kwargs how many top similar documents to return\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e10ac-3629-460b-a731-ea08c982e99d",
   "metadata": {},
   "source": [
    "### Initialize a Question answering langchain application with palm llm and context info. (via index to vector db)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26e729-825d-49b7-9901-31aafc054a06",
   "metadata": {},
   "source": [
    "- here chain_type = 'stuff' indicates how we need to pass the data into the prompt for the llm. i.e. stuff the context data into prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbdfafd0-73da-4fce-9762-31ada15185c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_qa_agent(retriever=None):\n",
    "    qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "    return qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bceef2-5519-40e8-8cb0-0071b8a89c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sm_ask(qa, question, print_results=True):\n",
    "\n",
    "  video_subset = qa({\"query\": question})\n",
    "  context = video_subset\n",
    "  prompt = f\"\"\"\n",
    "  Answer the following question in a detailed manner, using information from the text below. If the answer is not in the text,say I dont know and do not generate your own response.\n",
    "\n",
    "  Question:\n",
    "  {question}\n",
    "  Text:\n",
    "  {context}\n",
    "\n",
    "  Question:\n",
    "  {question}\n",
    "\n",
    "  Answer:\n",
    "  \"\"\"\n",
    "  parameters = {\n",
    "      \"temperature\": 0.1,\n",
    "      \"max_output_tokens\": 256,\n",
    "      \"top_p\": 0.8,\n",
    "      \"top_k\": 40\n",
    "  }\n",
    "  response = llm.predict(prompt, **parameters)\n",
    "  return {\n",
    "      \"answer\": response\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb18ba-d8df-4eae-a3f3-a656d1fe70ca",
   "metadata": {},
   "source": [
    "### Using gradio we generate a simple UI for our Question Answering application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9165f3c-adf3-4a60-90ec-fa1b0c1cf2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def get_response(url,input_text):\n",
    "    \n",
    "    # load video transcripts\n",
    "    transcript = load_video_transcripts(url)\n",
    "    # get embeddings for transcripts\n",
    "    docs, embedding_list = get_transcript_embeddings(transcript)\n",
    "    # index embeddings (context data) in vector db\n",
    "    retriever = index_and_retrieve(docs, embeddings)\n",
    "    # get qa agent\n",
    "    qa = get_qa_agent(retriever)\n",
    "    \n",
    "    response = sm_ask(qa, input_text)\n",
    "    \n",
    "    return response\n",
    "\n",
    "grapp = gr.Interface(fn=get_response, inputs=[\"text\",\"text\"], outputs=\"text\")\n",
    "grapp.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0551d-0340-4df8-aeec-b04e4a4ac74d",
   "metadata": {},
   "source": [
    "### sample questions we can use to test the above:\n",
    "\n",
    "- What is this video about and who are the speakers?\n",
    "- How is the tone and sentiment of this video?\n",
    "- Who are the target audience for this video?\n",
    "- Is this a marketing video by Google Cloud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062151e-0108-4105-a83b-3078c096be9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m114"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
